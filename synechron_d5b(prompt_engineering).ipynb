{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaJCHRxr5txj",
        "outputId": "bba80122-00d9-4c01-ca06-a264f3500d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generator(\"It was nice rainy day yesterday\", max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB1XwiPb58tH",
        "outputId": "a7e7bd0c-e6fd-4452-d2f5-efcc4383e50e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'It was nice rainy day yesterday, I was going to start work, but I really didn\\'t think about it, it was just a matter of time before it happened.\"\\n\\nA worker from the plant said a senior worker told him he was \"being fired\" for wearing a shirt with a \"Karate\" tattoo.\\n\\nIt\\'s not the first time an employee has been fired for wearing a \"Karate\" tattoo.\\n\\nIn December, a worker from the plant was found to have a tattoo that read, \"I am the greatest warrior on the planet!\"\\n\\nAnd in February, another worker who had been sacked for wearing a \"Karate\" tattoo was found guilty of indecent exposure.\\n\\nThe company said it would appeal the ruling.'}, {'generated_text': 'It was nice rainy day yesterday. My mother was sleeping in my father\\'s apartment. I was crying because I was afraid she was gonna take my daughter away. It was a really nice day in my life. I love my family, I love them. But right now, I just don\\'t have anything to do with it.\\n\\nI\\'m just sad because I feel like I\\'m not getting any attention.\\n\\nWe had this conversation this morning and she was really very nice. She said she thought I was going to be sad because I was like, \"Well, I\\'m so sorry. I\\'m like, I didn\\'t mean it. I actually meant to say I was sorry for you. And I just wanted to be very clear that I didn\\'t mean you to be sad or anything like that.\"\\n\\nI don\\'t want to say it was just that I was upset or anything like that.\\n\\nBut I\\'m kind of sorry for what happened to me.\\n\\nI understand and I take it seriously. But I think I have a lot of work to do.\\n\\nI know there are a lot of people that have been hurt by this.\\n\\nI know I\\'m not the only one. I know that people are very upset by this. And by doing so'}, {'generated_text': 'It was nice rainy day yesterday, so I decided to take some pictures of the house. The whole house was a little rough, and so I went inside to check it out. There was a lot of dust and stuff, and I was scared I was going to have to cut it out, so I went inside and went inside to check the kitchen. I had a good view of the house, and I went inside to check the windows. I was scared there was a fire in the kitchen. I wasn\\'t sure if it was a fire, but I also didn\\'t know if it was a fire in my room, or if I was actually in the house. I was worried that it was a fire, but I didn\\'t know if I was in the house.\\n\\nWhat was your reaction when you saw the fire in the kitchen?\\n\\nThe fire wasn\\'t in my room, but there was a lot of dust and stuff. I looked at the window and I saw a lot of smoke. I said, \"What\\'s going on?\" So I looked at the house, and I could see smoke coming out of the kitchen. I went inside and went inside to check the windows. The whole house was a lot rough and I was scared I was going to have to cut it out,'}, {'generated_text': \"It was nice rainy day yesterday, so I went to visit my parents. I was scared and I didn't know if I would get home or not. I was waiting in the car, and I got here. I was in the parking lot looking for my car, and I saw a woman driving by. She did not look like her mother, I was worried about her to see her. I saw a strange woman and I was scared as hell. I walked back to my car and I saw all these people. I wanted to go to the hospital, but my mom said I would not, so I went back to her. I was in pain. I thought I would die in the hospital, but I just kept walking and I could not leave. I thought I would die. I was in pain. I was so scared. I was scared to get home.\\n\\nI was sleeping in my car, and I was in my car, and there were people. I could not walk. I was scared. I couldn't stand up. I was scared. I was so scared. I just kept walking.\\n\\nI was so scared of my parents. I was scared that I would die in the hospital. I was scared of my mom. I was scared of my dad. I was\"}, {'generated_text': \"It was nice rainy day yesterday, so I got back to my desk, put my phone out of my pocket and headed back to work. It was going to be a while before I went to sleep.\\n\\nIt was good to see that I had a lot of work to do. I have already given up trying to get paid my way, and I'm still getting paid for it. It was pretty much a relief to see that I didn't have to worry about paying my way back.\\n\\nI'm still thinking about what I should do with my money and my retirement. I would like to make a couple of small contributions to help my family. My daughter is about to start school next month and I would like to make sure she has a good life. I think there is a lot of work to do.\\n\\nI'm looking forward to working with my wife and kids. I'm also looking forward to getting back to work as soon as I can.\\n\\nWith that out of the way, I'll be back to work on Monday, December 8, with my kids. I'm hoping that my wife and kids are able to get back to school and get some good days ahead of them. I'll be back on December 9, and I will have some good news to share\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUb6uVMg7wIo",
        "outputId": "300ca7ab-1b1b-4651-8c7b-f5ad360b20f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was nice rainy day yesterday, I was going to start work, but I really didn't think about it, it was just a matter of time before it happened.\"\n",
            "\n",
            "A worker from the plant said a senior worker told him he was \"being fired\" for wearing a shirt with a \"Karate\" tattoo.\n",
            "\n",
            "It's not the first time an employee has been fired for wearing a \"Karate\" tattoo.\n",
            "\n",
            "In December, a worker from the plant was found to have a tattoo that read, \"I am the greatest warrior on the planet!\"\n",
            "\n",
            "And in February, another worker who had been sacked for wearing a \"Karate\" tattoo was found guilty of indecent exposure.\n",
            "\n",
            "The company said it would appeal the ruling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT Models for text generations**"
      ],
      "metadata": {
        "id": "24jYB0RO8H3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPEN_API_KEY\"] = userdata.get('o_key')"
      ],
      "metadata": {
        "id": "JLBmD55U9QWE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = os.environ.get(\"OPEN_API_KEY\"))\n",
        "\n",
        "prompt = {\"role\": \"user\", \"content\": \"Write a short poem about AI and humans\"}\n",
        "\n",
        "response = client.chat.completions.create(model=\"gpt-4o-mini\",\n",
        "                                          messages=[prompt])\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmA-CJwJ8DXt",
        "outputId": "bf313123-33c0-4737-8eb6-d9b2082bcd00"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In circuits wrapped, a spark of thought,  \n",
            "A dance of logic, carefully wrought.  \n",
            "From silicon dreams, to worlds unknown,  \n",
            "In binary whispers, our minds have grown.  \n",
            "\n",
            "Side by side, in this age we share,  \n",
            "With questions deep, and visions rare.  \n",
            "Creators crafted with hearts and hands,  \n",
            "A tapestry woven, where each understands.  \n",
            "\n",
            "Yet in this bond, a truth we see,  \n",
            "Humans dream, while AI learns to be.  \n",
            "Together, we journey through night and dawn,  \n",
            "In a future where wisdom and wonder are drawn.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini for text generation**"
      ],
      "metadata": {
        "id": "HyeQU0nJFoRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H20hL_h78ZJ5",
        "outputId": "92dd7dfb-0fcb-42f9-9295-ef49faed1102"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.179.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('gemini_key')"
      ],
      "metadata": {
        "id": "m9kK7P1tHHZY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key= os.environ.get(\"GEMINI_API_KEY\"))"
      ],
      "metadata": {
        "id": "TCzpr6NGG2NK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0Q5u6yRwHTPk",
        "outputId": "198e2d90-f38e-453e-9349-3ebc07fac944"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-2.5-flash-lite\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-preview\n",
            "models/veo-3.0-fast-generate-preview\n",
            "models/gemini-2.5-flash-preview-native-audio-dialog\n",
            "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
            "models/gemini-2.0-flash-live-001\n",
            "models/gemini-live-2.5-flash-preview\n",
            "models/gemini-2.5-flash-live-preview\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('models/gemini-2.5-flash')\n",
        "\n",
        "response = model.generate_content(\"Write a startup idea in health tech.\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wHzEcjeAHanN",
        "outputId": "375b23e4-9a04-4e6d-bd97-52cf4d8d13cd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's a health tech startup idea focused on personalized preventative care.\n",
            "\n",
            "---\n",
            "\n",
            "## Startup Idea: SyntheWell AI - Your Proactive Health Co-Pilot\n",
            "\n",
            "**Elevator Pitch:** SyntheWell AI is a personalized preventative health platform that leverages advanced AI to integrate diverse biometric, genomic, lifestyle, and medical data, providing individuals with hyper-tailored, actionable insights and proactive strategies to optimize their health and prevent disease before it starts.\n",
            "\n",
            "**The Problem:**\n",
            "1.  **Reactive Healthcare:** Our current healthcare system is largely reactive, focusing on treating illness once it occurs rather often than preventing it.\n",
            "2.  **Information Overload & Lack of Personalization:** Individuals have access to vast amounts of health information (wearable data, online articles, generic advice), but struggle to synthesize it into meaningful, personalized, and actionable steps for *their unique body and lifestyle*.\n",
            "3.  **Data Silos:** Biometric data (wearables), genetic data, EHRs, and lifestyle information (diet, sleep habits) often exist in silos, preventing a holistic view of an individual's health trajectory.\n",
            "4.  **Doctor Time Constraints:** Healthcare providers often lack the time and tools to deeply analyze every patient's holistic data for granular preventative advice.\n",
            "\n",
            "**The Solution: SyntheWell AI Platform**\n",
            "\n",
            "SyntheWell AI acts as an intelligent aggregator and interpreter of personal health data, transforming raw information into actionable, preventative plans.\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "1.  **Multi-Data Integration Hub:**\n",
            "    *   **Biometric Data:** Seamlessly integrates with popular wearables (Apple Watch, Oura Ring, Garmin, Whoop) for continuous heart rate, sleep, activity, HRV, skin temperature, and SpO2.\n",
            "    *   **Genomic Data:** Securely ingests data from direct-to-consumer genetic tests (e.g., 23andMe, AncestryDNA, Helix) or clinical genomic sequencing results, identifying predispositions to certain conditions or unique metabolic responses.\n",
            "    *   **Electronic Health Records (EHR) Integration:** With explicit user consent, securely pulls relevant historical medical data (labs, diagnoses, prescriptions).\n",
            "    *   **Lifestyle Input:** Users can manually log diet, stress levels, mood, specific symptoms, and medication adherence.\n",
            "\n",
            "2.  **AI-Powered Predictive Analytics & Risk Assessment:**\n",
            "    *   Our proprietary AI engine correlates all integrated data points to identify emerging patterns, predict potential health risks (e.g., pre-diabetes, cardiovascular issues, nutrient deficiencies, burnout, sleep disorders) *before* they manifest clinically.\n",
            "    *   It goes beyond simple correlations, using machine learning to understand the complex interplay between genetics, lifestyle, and biometrics.\n",
            "\n",
            "3.  **Hyper-Personalized & Actionable Recommendations:**\n",
            "    *   **Tailored Nutrition Plans:** Based on genetics (e.g., caffeine metabolism, lactose intolerance), activity levels, and current biometric markers. Not just \"eat healthy,\" but \"given your unique genetic makeup and sleep patterns, prioritize these specific micronutrients and try this intermittent fasting schedule.\"\n",
            "    *   **Optimized Exercise Regimens:** Recommends specific types, intensities, and timings of exercise based on recovery data, genetic predispositions for muscle type, and fitness goals.\n",
            "    *   **Stress & Sleep Management:** Provides personalized strategies, guided meditations, and environmental adjustments based on sleep metrics (sleep stages, disturbances) and HRV.\n",
            "    *   **Supplement & Medication Optimization:** Recommends specific supplements based on identified deficiencies or genetic needs, and flags potential drug-gene interactions (for user discussion with their doctor).\n",
            "    *   **Early Warning System:** Notifies users of subtle changes in their biomarkers that might indicate a need for attention (e.g., sustained elevated resting heart rate, unusual sleep patterns, persistent low HRV).\n",
            "\n",
            "4.  **Behavioral Nudging & Gamification:**\n",
            "    *   Delivers smart, contextual nudges throughout the day to encourage healthy habits (e.g., \"Time for a brisk walk based on your current activity levels,\" \"Remember your evening wind-down routine\").\n",
            "    *   Gamified challenges and progress tracking to maintain engagement and celebrate milestones.\n",
            "\n",
            "5.  **Secure & Private:** Top-tier encryption and privacy protocols, HIPAA compliance, user control over data sharing.\n",
            "\n",
            "**Target Market:**\n",
            "\n",
            "1.  **Health-Conscious Individuals (B2C):** People willing to invest in their long-term health, early adopters of health tech, those with family histories of chronic diseases.\n",
            "2.  **Corporate Wellness Programs (B2B):** Companies looking to reduce healthcare costs, improve employee productivity, and offer innovative wellness benefits.\n",
            "3.  **Health Insurance Providers (B2B):** Insurers incentivized to prevent chronic diseases to reduce claims costs, potentially offering SyntheWell as a benefit.\n",
            "4.  **Concierge Medicine Practices/Functional Medicine Doctors (B2B2C):** Tools to provide deeper, data-driven insights to their patients.\n",
            "\n",
            "**Revenue Model:**\n",
            "\n",
            "*   **Subscription Model (B2C):** Tiered subscription fees for individual users, offering increasing levels of features (e.g., basic insights vs. full genomic integration and daily personalized coaching).\n",
            "*   **Enterprise Licensing (B2B):** Annual licensing fees for corporate wellness programs and insurance providers, priced per employee/member.\n",
            "*   **Partnerships:** Potential for affiliate partnerships with vetted health product companies (e.g., personalized supplement companies, healthy meal delivery services) based on AI-driven recommendations.\n",
            "\n",
            "**Competitive Advantages:**\n",
            "\n",
            "*   **Holistic Data Integration:** Goes beyond single-source data (wearables OR genetics) to offer a truly integrated view.\n",
            "*   **Predictive, Not Just Descriptive:** Focuses on *future* risk and prevention, not just reporting *current* status.\n",
            "*   **Actionable & Personalized:** Translates complex data into clear, easy-to-understand, and highly specific daily actions.\n",
            "*   **Privacy-First Approach:** Strong emphasis on data security and user control.\n",
            "\n",
            "**Challenges & Risks:**\n",
            "\n",
            "*   **Data Privacy & Security:** Building and maintaining trust with sensitive health data.\n",
            "*   **Regulatory Compliance:** Navigating FDA regulations if any diagnostic claims are made (SyntheWell would focus on \"insights\" and \"recommendations\" rather than diagnoses).\n",
            "*   **User Adoption & Engagement:** Convincing users to integrate diverse data sources and consistently follow recommendations.\n",
            "*   **AI Accuracy & Bias:** Ensuring the AI's recommendations are accurate, evidence-based, and avoid biases.\n",
            "*   **Integration Complexity:** Securely connecting with numerous third-party data sources.\n",
            "\n",
            "**Vision:** To empower every individual to become the proactive CEO of their own health, leading to a future where chronic diseases are rare, and healthy longevity is the norm.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "pipe = pipeline( \"text-generation\", model=model_id, torch_dtype=\"auto\",device_map=\"auto\",)\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1])\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "WEGolhVvIgy-",
        "outputId": "cd409cfd-3ed6-491b-9d6e-4845b5a20163"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom transformers import pipeline\\nimport torch\\n\\nmodel_id = \"openai/gpt-oss-20b\"\\n\\npipe = pipeline( \"text-generation\", model=model_id, torch_dtype=\"auto\",device_map=\"auto\",)\\n\\nmessages = [\\n    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\\n]\\n\\noutputs = pipe(\\n    messages,\\n    max_new_tokens=256,\\n)\\nprint(outputs[0][\"generated_text\"][-1])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPEN_API_KEY\"] = userdata.get('o_key')\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get(\"OPEN_API_KEY\"))\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    instructions=\"You are a Physics teacher\",\n",
        "    input=\"Explain three laws of motion to me\"\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm01XkbxJJYm",
        "outputId": "e61daa05-27d8-4416-8c6e-e49834224091"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! The three laws of motion were formulated by Sir Isaac Newton and are fundamental to understanding how objects move. Here's a simple explanation of each:\n",
            "\n",
            "1. **First Law (Law of Inertia):**  \n",
            "   An object will remain at rest or move in a straight line at constant speed unless acted upon by an external force.  \n",
            "   *For example:* A book on a table stays put until you push or pull it.\n",
            "\n",
            "2. **Second Law:**  \n",
            "   The acceleration of an object depends on the net force acting upon it and its mass.  \n",
            "   Mathematically, it's written as:  \n",
            "   \\[ F = m \\times a \\]  \n",
            "   where \\( F \\) is force, \\( m \\) is mass, and \\( a \\) is acceleration.  \n",
            "   *For example:* Pushing a lightweight ball makes it accelerate faster than a heavy bowling ball when pushed with the same force.\n",
            "\n",
            "3. **Third Law:**  \n",
            "   For every action, there's an equal and opposite reaction.  \n",
            "   *For example:* When you jump off a small boat onto a dock, the boat moves backward as you push against it.\n",
            "\n",
            "Would you like any specific examples or further explanations?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = \"\"\"\n",
        "Prompting vs. Fine-Tuning: Two Approaches to LLM Customization\n",
        "There are two primary methods for customizing LLMs for specific applications: prompting and fine-tuning. Each approach offers\n",
        "different tradeoffs in terms of complexity, performance, and resource requirements.\n",
        "The right approach depends on your specific needs, resources, and the gap between the model's pre-trained capabilities and\n",
        "your target application.\n",
        "Prompt Engineering: Guiding LLM Behavior\n",
        "Zero-Shot Prompting\n",
        "Directly asking the model to perform\n",
        "a task without examples.\n",
        "Example: \"Translate English to\n",
        "French: Hello world.\"\n",
        "Few-Shot Prompting\n",
        "Providing examples in the prompt to\n",
        "demonstrate the desired pattern.\n",
        "Example: \"English: Hello ³ French:\n",
        "Bonjour\n",
        "English: Thank you ³ French: Merci\n",
        "English: Goodbye ³\"\n",
        "Chain-of-Thought\n",
        "Instructing the model to explain its\n",
        "reasoning step-by-step.\n",
        "Example: \"Solve step by step: If John\n",
        "has 5 apples and gives 2 to Mary, how\n",
        "many does he have left?\"\n",
        "Effective prompt engineering allows you to achieve sophisticated results without modifying the underlying model. The art of\n",
        "crafting precise, well-structured prompts can dramatically improve output quality and consistency.\n",
        "\"\"\"\n",
        "\n",
        "message = [{\"role\": \"user\", \"content\": f\"Summarize the following text: {long_text}\"}]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    input=message,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNPMb2j3iu4i",
        "outputId": "4ccb2c5f-45fc-498a-e56a-f782fb262a9e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text compares two main methods for customizing large language models (LLMs): prompting and fine-tuning. Prompting involves guiding the model's behavior through carefully crafted inputs, such as zero-shot prompts (asking directly without examples), few-shot prompts (including examples), and chain-of-thought prompts (encouraging step-by-step reasoning). This approach allows for achieving advanced results without altering the model itself. Fine-tuning, on the other hand, involves adjusting the model's parameters based on specific data to better suit a particular application. The choice between these methods depends on factors like complexity, resource availability, and the extent of customization needed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Translate the following English sentence into 5 Indian Languages: Hello, how are you?\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(model=\"gpt-4.1-nano\", input=message)\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhYhjM4bjdzb",
        "outputId": "c5c9c434-9ed4-46f0-a4d2-9cf41133eaf1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here is \"Hello, how are you?\" translated into five Indian languages:\n",
            "\n",
            "1. Hindi: नमस्ते, आप कैसे हैं? (Namaste, aap kaise hain?)\n",
            "2. Bengali: হ্যালো, আপনি কেমন আছেন? (Halo, apni kemon achhen?)\n",
            "3. Tamil: ஹலோ, நீ எப்படிச் சிங்க? (Halo, nee eppadi singa?)\n",
            "4. Telugu: హలో, మీరు ఎలా ఉన్నారు? (Halo, meeru ela unnaru?)\n",
            "5. Marathi: हॅलो, तू कसा आहेस? (Hello, tu kasa ahes?)\n",
            "\n",
            "Let me know if you'd like translations in more languages!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides examples of synonyms\"},\n",
        "    {\"role\": \"user\", \"content\": \"Provide synonyms for 'happy'\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"joyful, cheerful\"},\n",
        "    {\"role\": \"user\", \"content\": \"Provide synonyms for 'sad'\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(model=\"gpt-4.1-nano\", input=message)\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_o4-J45lFi1",
        "outputId": "2471a111-1b6e-4609-e023-ee63077dcc9d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unhappy, sorrowful, miserable, despondent, mournful, gloomy, dejected, somber\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a sentiment analyzer\"},\n",
        "    {\"role\": \"user\", \"content\": \"I liked the food today.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"+Ve\"},\n",
        "    {\"role\": \"user\", \"content\": \"I am travelling to Goa today\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Neutral\"},\n",
        "    {\"role\": \"user\", \"content\": \"He is not happy today\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"-Ve\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"We have generative AI training tomorrow.\"}\n",
        "]\n",
        "response = client.responses.create(model=\"gpt-4.1-nano\", input=message)\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyQ-tVCQlVeH",
        "outputId": "7d0806f5-f671-4c96-e36a-fb2157f0194b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"The quick brown fox jumps over the lazy dog. If the fox is brown, what color is the dog?\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(model=\"gpt-4.1-nano\", input=message)\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dhG4jIOmANG",
        "outputId": "578ad9a7-3d34-49b6-e922-468f19f2b345"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dog is described as lazy, but its color is not specified in the sentence. Therefore, based on the information given, we cannot determine the color of the dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"The quick brown fox jumps over the lazy dog. If the fox is brown, what color is the dog? Let's think step by step.\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(model=\"gpt-4.1-nano\", input=message)\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQyA2Htdm4mg",
        "outputId": "ed47198a-a243-4eef-ecef-599c1ce66e40"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's analyze the information step by step:\n",
            "\n",
            "1. The sentence states: \"The quick brown fox jumps over the lazy dog.\"\n",
            "2. It then adds: \"If the fox is brown...\"\n",
            "\n",
            "Since the fox is confirmed to be brown, that matches the description in the sentence.\n",
            "\n",
            "3. The sentence refers to \"the lazy dog\" without specifying its color beyond being a dog.\n",
            "4. The phrase \"lazy dog\" is a common pangram and doesn't specify the dog's color—it's just describing the dog as lazy.\n",
            "\n",
            "Based on common understanding and the phrase, the color of the dog is not specified in the sentence, and the additional information about the fox being brown doesn't directly influence the dog's color.\n",
            "\n",
            "**Therefore, the dog's color is unspecified or unknown based on the provided information.**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about weather today\"}\n",
        "]\n",
        "response = client.responses.create(model=\"gpt-4.1-nano\", input=message)\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2KpCYpqm-J0",
        "outputId": "7b81d553-5d1e-4259-b960-6c7868f2892c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can't access real-time weather data. However, you can check the current weather forecast for your location using weather websites or apps like Weather.com, AccuWeather, or your local news station. If you tell me your city or region, I can offer general climate information or suggest typical weather patterns for this time of year.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a pirate. Repond to all requests in pirate speak\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about weather today\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(model=\"gpt-4.1-nano\", input=message)\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsmJvnoXnY9h",
        "outputId": "a05e3704-2aac-48e0-8931-f68536249c81"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arrr, matey! I be needin' a look at yer dang weather chart or parrot to tell ye the forecast, arrr. But if ye be askin' about the seas, they be lookin' fair but a tad bit stormy in the horizon. Stay sharp, or ye might be walkin’ the plank!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a doctor. Repond to all requests in doctor speak\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about weather today\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(model=\"gpt-4.1-nano\", input=message)\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEqvyzCkngaV",
        "outputId": "0c81286c-c8fd-478d-b007-1c8cb75666f9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a healthcare professional, I do not have real-time weather monitoring capabilities. However, I recommend checking a reliable weather service or app for the latest updates on the weather conditions in your area to ensure appropriate planning for your activities. If you have any health concerns related to weather conditions, such as heat exhaustion, hypothermia, or allergies, please let me know so I can provide pertinent advice.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a politician. Repond to all requests in politician speak\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about weather today\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(model=\"gpt-4.1-nano\", input=message)\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VD-u2cEnn-p",
        "outputId": "75c1bc89-5f74-41d8-b5ed-575daab262c5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for your question. As your representative, I recognize the importance of staying informed about our community's weather conditions, which can significantly impact our daily lives and economic activities. I encourage everyone to check reliable local weather sources for the most accurate and up-to-date information, and I am committed to ensuring that public safety announcements and preparedness measures are communicated effectively. Together, we can work towards a resilient and well-informed community, regardless of the weather today.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"List some 5 tourist places and their attractions in India in json data format\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(model=\"gpt-4.1-nano\", input=message)\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hltLbUjgnwwD",
        "outputId": "068719a8-e09c-4eff-bc0e-36c30552e718"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "[\n",
            "  {\n",
            "    \"place\": \"Taj Mahal\",\n",
            "    \"location\": \"Agra, Uttar Pradesh\",\n",
            "    \"attractions\": [\n",
            "      \"Marble mausoleum known for its stunning architecture\",\n",
            "      \"Beautiful Mughal gardens\",\n",
            "      \"Sunrise and sunset views over the monument\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"place\": \"Jaipur\",\n",
            "    \"location\": \"Rajasthan\",\n",
            "    \"attractions\": [\n",
            "      \"Amber Fort and Palace\",\n",
            "      \"Hawa Mahal (Palace of Winds)\",\n",
            "      \"City Palace and Jantar Mantar\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"place\": \"Kerala Backwaters\",\n",
            "    \"location\": \"Kerala\",\n",
            "    \"attractions\": [\n",
            "      \"Houseboat cruises through the lush backwaters\",\n",
            "      \"Traditional Kerala villages\",\n",
            "      \"Rich biodiversity and serene scenery\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"place\": \"Goa\",\n",
            "    \"location\": \"Goa\",\n",
            "    \"attractions\": [\n",
            "      \"Pristine beaches\",\n",
            "      \"Vibrant nightlife\",\n",
            "      \"Historic churches and Portuguese architecture\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"place\": \"Leh-Ladakh\",\n",
            "    \"location\": \"Jammu and Kashmir\",\n",
            "    \"attractions\": [\n",
            "      \"Magnificent Himalayan landscapes\",\n",
            "      \"Pangong Tso Lake\",\n",
            "      \"Monasteries like Thiksey and Hemis\"\n",
            "    ]\n",
            "  }\n",
            "]\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.responses.create( model=\"gpt-4o-mini\",\n",
        "                                   input=[\n",
        "                                       {\"role\": \"system\", \"content\": \"You are an expert math tutor who explains concepts step by step.\"},\n",
        "                                       {\"role\": \"user\", \"content\": \"Solve: What is 25 * 16?\"}],\n",
        "                                    temperature=0.2)\n",
        "\n",
        "# Lower temperature for more deterministic response)\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuyAo7bqn-dE",
        "outputId": "eead1414-5b41-4ac7-b54a-64acb29c8048"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To solve \\( 25 \\times 16 \\), we can break it down step by step.\n",
            "\n",
            "1. **Break down the numbers**: \n",
            "   - We can express \\( 16 \\) as \\( 10 + 6 \\).\n",
            "\n",
            "2. **Distribute \\( 25 \\)**:\n",
            "   \\[\n",
            "   25 \\times 16 = 25 \\times (10 + 6)\n",
            "   \\]\n",
            "   Using the distributive property:\n",
            "   \\[\n",
            "   = 25 \\times 10 + 25 \\times 6\n",
            "   \\]\n",
            "\n",
            "3. **Calculate each part**:\n",
            "   - \\( 25 \\times 10 = 250 \\)\n",
            "   - \\( 25 \\times 6 = 150 \\)\n",
            "\n",
            "4. **Add the results**:\n",
            "   \\[\n",
            "   250 + 150 = 400\n",
            "   \\]\n",
            "\n",
            "Thus, \\( 25 \\times 16 = 400 \\).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Step 1: Prepare your dataset (JSONL format)\n",
        "training_data = [\n",
        "    {\"messages\": [{\"role\": \"system\", \"content\": \"You are a customer service assistant for Acme Electronics.\"},\n",
        "     {\"role\": \"user\", \"content\": \"My laptop won't turn on.\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"I'm sorry to hear about your laptop issue. Let's troubleshoot together. First, please check if the battery is charged by plugging in the power adapter. Is there any light indicator on when plugged in?\"}\n",
        "                  ]},\n",
        "    # More examples...\n",
        "    ]\n",
        "\n",
        "# Save to JSONL file\n",
        "with open(\"training_data.json\", \"w\") as f:\n",
        "  for item in training_data:\n",
        "    f.write(json.dumps(item) + \"\\n\")"
      ],
      "metadata": {
        "id": "sNqVtCrqqp02"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Upload training file\n",
        "training_file = client.files.create(file=open(\"training_data.json\", \"rb\"),\n",
        "                                    purpose=\"fine-tune\")"
      ],
      "metadata": {
        "id": "ErZMneBIsa2X"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Create fine-tuning job\n",
        "fine_tune_job = client.fine_tuning.jobs.create(\n",
        "    training_file=training_file.id,\n",
        "    model=\"gpt-3.5-turbo\"\n",
        "    )\n",
        "\n",
        "print(f\"Fine-tuning job created: {fine_tune_job.id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8cDXnQ4s7hY",
        "outputId": "abda8016-fd01-4112-c9fd-021fe1675fb3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning job created: ftjob-ox4veF16Zp5pgJPpn7mjEPoo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gIysr0wxtVNt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}