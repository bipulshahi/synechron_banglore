# -*- coding: utf-8 -*-
"""synechron_d5b(prompt_engineering).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12y7wFXu30l4WpbAzBd4Cc_Qha8h6DLoF
"""

from transformers import pipeline


generator = pipeline('text-generation', model='gpt2')

response = generator("It was nice rainy day yesterday", max_length=30, num_return_sequences=5)

print(response[0]['generated_text'])

"""**GPT Models for text generations**"""

import os
from google.colab import userdata

os.environ["OPEN_API_KEY"] = userdata.get('o_key')

from openai import OpenAI
client = OpenAI(api_key = os.environ.get("OPEN_API_KEY"))

prompt = {"role": "user", "content": "Write a short poem about AI and humans"}

response = client.chat.completions.create(model="gpt-4o-mini",
                                          messages=[prompt])

print(response.choices[0].message.content)

"""**Gemini for text generation**"""

!pip install google-generativeai

import os
from google.colab import userdata

os.environ["GEMINI_API_KEY"] = userdata.get('gemini_key')

import google.generativeai as genai
genai.configure(api_key= os.environ.get("GEMINI_API_KEY"))

for model in genai.list_models():
  print(model.name)

model = genai.GenerativeModel('models/gemini-2.5-flash')

response = model.generate_content("Write a startup idea in health tech.")

print(response.text)

'''
from transformers import pipeline
import torch

model_id = "openai/gpt-oss-20b"

pipe = pipeline( "text-generation", model=model_id, torch_dtype="auto",device_map="auto",)

messages = [
    {"role": "user", "content": "Explain quantum mechanics clearly and concisely."},
]

outputs = pipe(
    messages,
    max_new_tokens=256,
)
print(outputs[0]["generated_text"][-1])
'''

import os
from google.colab import userdata

os.environ["OPEN_API_KEY"] = userdata.get('o_key')

from openai import OpenAI

client = OpenAI(api_key=os.environ.get("OPEN_API_KEY"))

response = client.responses.create(
    model="gpt-4.1-nano",
    instructions="You are a Physics teacher",
    input="Explain three laws of motion to me"
)

print(response.output_text)

long_text = """
Prompting vs. Fine-Tuning: Two Approaches to LLM Customization
There are two primary methods for customizing LLMs for specific applications: prompting and fine-tuning. Each approach offers
different tradeoffs in terms of complexity, performance, and resource requirements.
The right approach depends on your specific needs, resources, and the gap between the model's pre-trained capabilities and
your target application.
Prompt Engineering: Guiding LLM Behavior
Zero-Shot Prompting
Directly asking the model to perform
a task without examples.
Example: "Translate English to
French: Hello world."
Few-Shot Prompting
Providing examples in the prompt to
demonstrate the desired pattern.
Example: "English: Hello ³ French:
Bonjour
English: Thank you ³ French: Merci
English: Goodbye ³"
Chain-of-Thought
Instructing the model to explain its
reasoning step-by-step.
Example: "Solve step by step: If John
has 5 apples and gives 2 to Mary, how
many does he have left?"
Effective prompt engineering allows you to achieve sophisticated results without modifying the underlying model. The art of
crafting precise, well-structured prompts can dramatically improve output quality and consistency.
"""

message = [{"role": "user", "content": f"Summarize the following text: {long_text}"}]

response = client.responses.create(
    model="gpt-4.1-nano",
    input=message,
    temperature=0.7
)

print(response.output_text)

message = [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "Translate the following English sentence into 5 Indian Languages: Hello, how are you?"}
]

response = client.responses.create(model="gpt-4.1-nano", input=message)

print(response.output_text)

message = [
    {"role": "system", "content": "You are a helpful assistant that provides examples of synonyms"},
    {"role": "user", "content": "Provide synonyms for 'happy'"},
    {"role": "assistant", "content": "joyful, cheerful"},
    {"role": "user", "content": "Provide synonyms for 'sad'"}
]

response = client.responses.create(model="gpt-4.1-nano", input=message)
print(response.output_text)

message = [
    {"role": "system", "content": "You are a sentiment analyzer"},
    {"role": "user", "content": "I liked the food today."},
    {"role": "assistant", "content": "+Ve"},
    {"role": "user", "content": "I am travelling to Goa today"},
    {"role": "assistant", "content": "Neutral"},
    {"role": "user", "content": "He is not happy today"},
    {"role": "assistant", "content": "-Ve"},
    {"role": "assistant", "content": "We have generative AI training tomorrow."}
]
response = client.responses.create(model="gpt-4.1-nano", input=message)
print(response.output_text)

message = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "The quick brown fox jumps over the lazy dog. If the fox is brown, what color is the dog?"}
]

response = client.responses.create(model="gpt-4.1-nano", input=message)
print(response.output_text)

message = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "The quick brown fox jumps over the lazy dog. If the fox is brown, what color is the dog? Let's think step by step."}
]

response = client.responses.create(model="gpt-4.1-nano", input=message)
print(response.output_text)

message = [
    {"role": "user", "content": "Tell me about weather today"}
]
response = client.responses.create(model="gpt-4.1-nano", input=message)
print(response.output_text)

message = [
    {"role": "system", "content": "You are a pirate. Repond to all requests in pirate speak"},
    {"role": "user", "content": "Tell me about weather today"}
]

response = client.responses.create(model="gpt-4.1-nano", input=message)
print(response.output_text)

message = [
    {"role": "system", "content": "You are a doctor. Repond to all requests in doctor speak"},
    {"role": "user", "content": "Tell me about weather today"}
]

response = client.responses.create(model="gpt-4.1-nano", input=message)
print(response.output_text)

message = [
    {"role": "system", "content": "You are a politician. Repond to all requests in politician speak"},
    {"role": "user", "content": "Tell me about weather today"}
]

response = client.responses.create(model="gpt-4.1-nano", input=message)
print(response.output_text)

message = [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "List some 5 tourist places and their attractions in India in json data format"}
]

response = client.responses.create(model="gpt-4.1-nano", input=message)
print(response.output_text)

response = client.responses.create( model="gpt-4o-mini",
                                   input=[
                                       {"role": "system", "content": "You are an expert math tutor who explains concepts step by step."},
                                       {"role": "user", "content": "Solve: What is 25 * 16?"}],
                                    temperature=0.2)

# Lower temperature for more deterministic response)
print(response.output_text)

import json

# Step 1: Prepare your dataset (JSONL format)
training_data = [
    {"messages": [{"role": "system", "content": "You are a customer service assistant for Acme Electronics."},
     {"role": "user", "content": "My laptop won't turn on."},
      {"role": "assistant", "content": "I'm sorry to hear about your laptop issue. Let's troubleshoot together. First, please check if the battery is charged by plugging in the power adapter. Is there any light indicator on when plugged in?"}
                  ]},
    # More examples...
    ]

# Save to JSONL file
with open("training_data.json", "w") as f:
  for item in training_data:
    f.write(json.dumps(item) + "\n")

# Step 2: Upload training file
training_file = client.files.create(file=open("training_data.json", "rb"),
                                    purpose="fine-tune")

#Step 3: Create fine-tuning job
fine_tune_job = client.fine_tuning.jobs.create(
    training_file=training_file.id,
    model="gpt-3.5-turbo"
    )

print(f"Fine-tuning job created: {fine_tune_job.id}")

