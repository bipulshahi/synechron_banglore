{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "389dMM8yR0QX",
        "outputId": "d3ba84d1-c95c-47a4-9ab0-1691a0f74983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start at: 0\n",
            "Move 1: action=1, state=1, reward=-1\n",
            "Move 2: action=-1, state=0, reward=-1\n",
            "Move 3: action=1, state=1, reward=-1\n",
            "Move 4: action=-1, state=0, reward=-1\n",
            "Move 5: action=-1, state=0, reward=-1\n",
            "Move 6: action=1, state=1, reward=-1\n",
            "Move 7: action=-1, state=0, reward=-1\n",
            "Move 8: action=1, state=1, reward=-1\n",
            "Move 9: action=1, state=2, reward=-1\n",
            "Move 10: action=-1, state=1, reward=-1\n",
            "Total Reward: -10\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Define environment\n",
        "states = [0, 1, 2, 3, 4]   # positions in 1D world\n",
        "goal_state = 4\n",
        "\n",
        "def step(state, action):\n",
        "    \"\"\"Take an action (left=-1, right=+1) and return new state and reward\"\"\"\n",
        "    new_state = max(0, min(goal_state, state + action))  # keep within [0,4]\n",
        "    if new_state == goal_state:\n",
        "        return new_state, 10   # reached goal\n",
        "    else:\n",
        "        return new_state, -1   # penalty for step\n",
        "\n",
        "# Example run (random moves)\n",
        "state = 0\n",
        "total_reward = 0\n",
        "print(\"Start at:\", state)\n",
        "\n",
        "for i in range(10):\n",
        "    action = random.choice([-1, 1])  # move left or right randomly\n",
        "    state, reward = step(state, action)\n",
        "    total_reward += reward\n",
        "    print(f\"Move {i+1}: action={action}, state={state}, reward={reward}\")\n",
        "\n",
        "print(\"Total Reward:\", total_reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define environment\n",
        "states = [0, 1, 2, 3, 4]\n",
        "actions = [-1, 1]  # left, right\n",
        "goal_state = 4\n",
        "\n",
        "def step(state, action):\n",
        "    new_state = max(0, min(goal_state, state + action))\n",
        "    if new_state == goal_state:\n",
        "        return new_state, 10\n",
        "    else:\n",
        "        return new_state, -1\n",
        "\n",
        "# Q-table initialization\n",
        "Q = np.zeros((len(states), len(actions)))\n",
        "\n",
        "# Parameters\n",
        "alpha = 0.1      # learning rate\n",
        "gamma = 0.9      # discount factor\n",
        "epsilon = 0.2    # exploration (try random moves)\n",
        "\n",
        "# Training\n",
        "episodes = 1000\n",
        "for _ in range(episodes):\n",
        "    state = 0  # start\n",
        "    while state != goal_state:\n",
        "        # Choose action (explore or exploit)\n",
        "        if random.uniform(0,1) < epsilon:\n",
        "            action_idx = random.choice([0,1])\n",
        "        else:\n",
        "            action_idx = np.argmax(Q[state])\n",
        "        action = actions[action_idx]\n",
        "\n",
        "        # Take step\n",
        "        new_state, reward = step(state, action)\n",
        "\n",
        "        # Update Q-value\n",
        "        Q[state, action_idx] += alpha * (reward + gamma * np.max(Q[new_state]) - Q[state, action_idx])\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "print(\"Learned Q-table:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igUFV5SZR117",
        "outputId": "5258d255-2436-4e38-ddb8-e17fe6312850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-table:\n",
            "[[ 3.12197926  4.58      ]\n",
            " [ 3.12196216  6.2       ]\n",
            " [ 4.5799762   8.        ]\n",
            " [ 6.19996741 10.        ]\n",
            " [ 0.          0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is this Q-table?\n",
        "\n",
        "* Rows = states (0, 1, 2, 3, 4)\n",
        "* Columns = actions:\n",
        "\n",
        "  * Col 0 = action = **−1** (move left)\n",
        "  * Col 1 = action = **+1** (move right)\n",
        "* Each entry = expected future reward if the agent takes that action from that state, following the learned policy.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* `Q[0, 0] = 3.12` → If at **state 0** and you move left (which keeps you at 0), the expected future reward ≈ 3.1.\n",
        "* `Q[0, 1] = 4.58` → If at **state 0** and you move right, the expected future reward ≈ 4.6.\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Why do values increase as you move closer to the goal?\n",
        "\n",
        "Reading row by row:\n",
        "\n",
        "* **State 0**: `[3.12, 4.58]` → Going right (4.58) is better than left (3.12).\n",
        "* **State 1**: `[3.12, 6.20]` → Going right gives higher value.\n",
        "* **State 2**: `[4.58, 8.00]` → Right is much better.\n",
        "* **State 3**: `[6.20, 10.0]` → Going right reaches the goal (reward 10).\n",
        "* **State 4**: `[0, 0]` → Goal state: no actions matter anymore (episode ends).\n",
        "\n",
        "The numbers grow bigger as you get closer to the goal because the agent is looking ahead at **future discounted rewards**.\n",
        "\n",
        "---\n",
        "\n",
        "# 3. What does it mean in terms of policy?\n",
        "\n",
        "The best action per state is the column with the higher Q-value:\n",
        "\n",
        "* State 0 → best action = **+1 (right)**\n",
        "* State 1 → best action = **+1 (right)**\n",
        "* State 2 → best action = **+1 (right)**\n",
        "* State 3 → best action = **+1 (right)**\n",
        "\n",
        "So the **optimal policy** is: always move **right** until you reach the goal.\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Why aren’t left moves negative?\n",
        "\n",
        "Notice that even moving left has a positive number (e.g., 3.12).\n",
        "That’s because:\n",
        "\n",
        "* Even if you waste moves, eventually you’ll still reach the goal (+10).\n",
        "* But since you get −1 penalty per step, left is **worse** than right.\n",
        "\n",
        "So Q-values for left are lower than for right.\n",
        "\n",
        "---\n",
        "\n",
        "# 5. How to interpret numerically?\n",
        "\n",
        "Imagine starting at state 0:\n",
        "\n",
        "* If you always go right → shortest path = 4 steps → total reward ≈ (−1 −1 −1 +10) = **7**.\n",
        "* If you go left sometimes → you delay reaching the goal, so the total reward shrinks (extra −1 penalties).\n",
        "\n",
        "That’s why **right Q-values are higher**.\n",
        "\n",
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "* The Q-table tells us **how good each action is in each state**.\n",
        "* The agent has learned to **always move right** to maximize reward.\n",
        "* The values reflect the **expected total reward**, considering step penalties and the final goal.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "bxAEdEfFTKer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "state = state[0] if isinstance(state, tuple) else state\n",
        "\n",
        "env.render()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = np.argmax(Q_table[state, :])  # greedy (optimal) action\n",
        "    next_state, reward, done, *info = env.step(action)\n",
        "    env.render()\n",
        "    state = next_state\n",
        "\n",
        "print(\"Game finished with reward:\", reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LySRlXkhY0n-",
        "outputId": "d46562bd-1e68-48b0-8ea7-c868fe028d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.12/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Game finished with reward: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nMTgcpVuaXGl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}